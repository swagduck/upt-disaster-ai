import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from datetime import timedelta
import os
import joblib

# Import k·∫øt n·ªëi Database
from app.core.database import Database

class GuardianAI:
    def __init__(self):
        self.scaler = StandardScaler()
        # Random Forest: M·∫°nh m·∫Ω, ƒëa nƒÉng
        self.model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
        self.is_trained = False
        
        # Danh s√°ch t·ªça ƒë·ªô V√†nh ƒëai l·ª≠a (Ki·∫øn th·ª©c ƒë·ªãa l√Ω)
        self.fault_lines = [
            [36.2, 138.2], [37.7, -122.4], [-33.4, -70.6], 
            [-6.2, 106.8], [14.0, 121.0], [-41.2, 174.7], 
            [35.0, 25.0], [28.0, 84.0]
        ]
        
        # Kh·ªüi t·∫°o buffer
        self.X_buffer = []
        self.y_buffer = []
        
        # [QUY TR√åNH KH·ªûI ƒê·ªòNG TH√îNG MINH]
        # 1. Th·ª≠ k·∫øt n·ªëi DB v√† h·ªçc t·ª´ l·ªãch s·ª≠
        if Database.db is not None:
            print("‚è≥ [NEURAL CORE] Mining historical data from MongoDB...")
            count = self.train_from_history()
            
            if count > 10:
                print(f"üß† [NEURAL CORE] Trained on {count} historical snapshots (Time-Travel Mode).")
            else:
                # 2. N·∫øu kh√¥ng c√≥ l·ªãch s·ª≠, ch·∫°y ch·∫ø ƒë·ªô ch·ªù (Safe Mode)
                self._init_safe_mode()
        else:
            self._init_safe_mode()

    def _get_distance_to_fault(self, lat, lon):
        """T√≠nh kho·∫£ng c√°ch ƒë·∫øn ƒëi·ªÉm ƒë·ª©t g√£y g·∫ßn nh·∫•t"""
        min_dist = 99999.0
        for f_lat, f_lon in self.fault_lines:
            # 1 ƒë·ªô ~ 111km
            dist = np.sqrt((lat - f_lat)**2 + (lon - f_lon)**2) * 111.0
            if dist < min_dist: min_dist = dist
        return min_dist

    def _init_safe_mode(self):
        """
        [SAFE MODE] Kh·ªüi t·∫°o m√¥ h√¨nh ·ªü tr·∫°ng th√°i 'r·ªóng' nh∆∞ng kh√¥ng l·ªói.
        D√πng vector zero ƒë·ªÉ fit Scaler.
        """
        print("‚ö†Ô∏è [NEURAL CORE] No history found. Running in SAFE MODE (Waiting for data).")
        # Vector 5 chi·ªÅu r·ªóng: [Lat, Lon, Energy, Anomaly, Dist]
        self.X_buffer = [[0.0, 0.0, 0.0, 0.0, 0.0]]
        self.y_buffer = [0.0]
        
        self.scaler.fit(self.X_buffer)
        self.model.fit(self.scaler.transform(self.X_buffer), self.y_buffer)
        self.is_trained = False 

    def train_from_history(self):
        """
        H·ªçc t·ª´ qu√° kh·ª©: Input(T) -> Output(T+24h)
        """
        col = Database.get_collection("raw_logs")
        
        # Ki·ªÉm tra collection t·ªìn t·∫°i an to√†n
        if col is None: return 0
        
        try:
            # L·∫•y 1000 b·∫£n ghi g·∫ßn nh·∫•t
            logs = list(col.find().sort("timestamp", 1).limit(1000))
        except Exception as e:
            print(f"‚ö†Ô∏è DB Read Error: {e}")
            return 0

        if len(logs) < 5: return 0 
        
        X_history = []
        y_history = []
        
        for i in range(len(logs)):
            current_log = logs[i]
            current_time = current_log.get('timestamp')
            if not current_time: continue

            # T√¨m s·ª± ki·ªán l·ªõn nh·∫•t trong 24h t·ªõi
            future_max_mag = 0.0
            found_future = False
            
            for j in range(i + 1, len(logs)):
                future_log = logs[j]
                future_time = future_log.get('timestamp')
                if not future_time: continue

                time_diff = (future_time - current_time).total_seconds()
                if time_diff > 24 * 3600: break # Ch·ªâ nh√¨n xa 24h
                
                mag = future_log.get('max_magnitude', 0)
                if mag > future_max_mag:
                    future_max_mag = mag
                    found_future = True
            
            if not found_future: continue 

            sensors = current_log.get('sensors_data', [])
            if not sensors: continue
            
            # L·∫•y m·∫´u c√°c tr·∫°m ƒë·ªÉ train (Gi·ªõi h·∫°n 20 tr·∫°m/log ƒë·ªÉ nhanh)
            for s in sensors[:20]: 
                if 'lat' not in s or 'lon' not in s: continue
                
                dist = self._get_distance_to_fault(s['lat'], s['lon'])
                
                X_history.append([
                    s['lat'], s['lon'], 
                    s.get('energy_level', 0), 
                    s.get('anomaly_score', 0),
                    dist
                ])
                
                # Label: Chu·∫©n h√≥a Magnitude v·ªÅ [0,1]
                target_risk = min(1.0, future_max_mag / 9.0)
                y_history.append(target_risk)

        if not X_history: return 0
        
        # Retrain th·∫≠t
        X = np.array(X_history)
        y = np.array(y_history)
        
        self.scaler.fit(X)
        self.model.fit(self.scaler.transform(X), y)
        self.is_trained = True
        
        return len(X_history)

    def learn(self, sensors_data):
        # Level 2: Ch·ªß y·∫øu h·ªçc t·ª´ DB (train_from_history).
        # H√†m n√†y c√≥ th·ªÉ ƒë·ªÉ tr·ªëng ho·∫∑c d√πng ƒë·ªÉ t√≠ch l≈©y buffer RAM t·∫°m th·ªùi.
        return 0 
        
    def predict_risk(self, lat, lon, energy, anomaly):
        """
        D·ª± b√°o r·ªßi ro. N·∫øu ch∆∞a train xong (Safe Mode) th√¨ d√πng c√¥ng th·ª©c t·∫°m.
        """
        # Fallback Logic
        if not self.is_trained:
            # C√¥ng th·ª©c c≈© t·∫°m th·ªùi
            return (energy * 0.7) + (anomaly * 0.3)
        
        try:
            dist = self._get_distance_to_fault(lat, lon)
            input_data = np.array([[lat, lon, energy, anomaly, dist]])
            scaled_input = self.scaler.transform(input_data)
            prediction = self.model.predict(scaled_input)[0]
            return float(max(0.0, min(1.0, prediction)))
        except Exception:
            return 0.0

guardian_brain = GuardianAI()