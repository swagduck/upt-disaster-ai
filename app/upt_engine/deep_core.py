import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from collections import deque
from app.core.database import Database

class DeepGuardian:
    def __init__(self):
        self.model_path = "app/upt_engine/guardian_lstm.keras"
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.look_back = 5 # AI nh√¨n l·∫°i 5 b∆∞·ªõc th·ªùi gian
        self.is_trained = False
        self.model = None
        
        # --- BUFFER B·ªò NH·ªö TH·ª∞C T·∫æ ---
        # L∆∞u tr·ªØ d√≤ng d·ªØ li·ªáu realtime ƒë·ªÉ t·∫°o sequence cho LSTM
        self.realtime_buffer = deque(maxlen=self.look_back)
        
        # Ki·ªÉm tra GPU
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"üöÄ [DEEP CORE] NVIDIA GPU Active: {len(gpus)} device(s).")
        else:
            print("‚ö†Ô∏è [DEEP CORE] Running on CPU Mode.")

        self._build_brain()
        
        # Kh·ªüi ƒë·ªông: Train t·ª´ DB
        if Database.db is not None:
            print("üß† [DEEP CORE] Loading Memory Patterns...")
            self.train_from_memory()
        
    def _build_brain(self):
        """LSTM Architecture"""
        self.model = tf.keras.models.Sequential()
        self.model.add(tf.keras.layers.Input(shape=(self.look_back, 5)))
        
        # T·∫ßng LSTM s√¢u h∆°n ƒë·ªÉ b·∫Øt pattern ph·ª©c t·∫°p
        self.model.add(tf.keras.layers.LSTM(units=64, return_sequences=True))
        self.model.add(tf.keras.layers.Dropout(0.2))
        
        self.model.add(tf.keras.layers.LSTM(units=32))
        self.model.add(tf.keras.layers.Dropout(0.2))
        
        self.model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # Output 0-1 (Risk Score)
        
        self.model.compile(optimizer='adam', loss='binary_crossentropy')

    def train_from_memory(self):
        """Train l·∫°i model t·ª´ l·ªãch s·ª≠ MongoDB"""
        col = Database.get_collection("raw_logs")
        if col is None: return 0
        
        try:
            logs = list(col.find().sort("timestamp", 1).limit(1000))
        except: return 0
        
        if len(logs) < self.look_back + 10: return 0
        
        data = []
        for log in logs:
            # Feature Extraction
            sensors = log.get('sensors_data', [])
            avg_energy = np.mean([s.get('energy_level', 0) for s in sensors]) if sensors else 0
            avg_anomaly = np.mean([s.get('anomaly_score', 0) for s in sensors]) if sensors else 0
            # Target gi·∫£ ƒë·ªãnh: N·∫øu max_magnitude > 5.0 th√¨ l√† High Risk (1.0)
            max_mag = log.get('max_magnitude', 0)
            
            # Vector [Energy, Anomaly, Mag, Flux(Mock), RandomBias]
            data.append([avg_energy, avg_anomaly, max_mag, 0.5, 0.5])

        dataset = np.array(data)
        # Fit scaler
        self.scaler.fit(dataset)
        dataset_scaled = self.scaler.transform(dataset)
        
        X, y = [], []
        for i in range(self.look_back, len(dataset_scaled)):
            X.append(dataset_scaled[i-self.look_back:i, :])
            # Target: N·∫øu Mag > 0.5 (sau scale) th√¨ Risk = 1
            y.append(dataset_scaled[i, 2]) 
            
        X, y = np.array(X), np.array(y)
        
        self.model.fit(X, y, epochs=3, batch_size=4, verbose=0)
        self.is_trained = True
        return len(X)

    def predict_risk(self, lat, lon, energy, anomaly):
        """
        D·ª± ƒëo√°n r·ªßi ro d·ª±a tr√™n chu·ªói d·ªØ li·ªáu th·ª±c t·∫ø (Real-time Sequence).
        """
        # T·∫°o vector feature hi·ªán t·∫°i
        # [Energy, Anomaly, PlaceholderMag, PlaceholderFlux, PlaceholderBias]
        current_features = [energy, anomaly, 0.5, 0.5, 0.5]
        
        # 1. C·∫≠p nh·∫≠t b·ªô nh·ªõ ng·∫Øn h·∫°n
        self.realtime_buffer.append(current_features)
        
        # N·∫øu ch∆∞a ƒë·ªß d·ªØ li·ªáu l·ªãch s·ª≠ (l√∫c m·ªõi kh·ªüi ƒë·ªông), d√πng thu·∫≠t to√°n th√¥
        if len(self.realtime_buffer) < self.look_back:
            return (energy * 0.7 + anomaly * 0.3)
            
        if not self.is_trained:
            return (energy + anomaly) / 2.0

        try:
            # 2. Chu·∫©n b·ªã Input cho LSTM
            # L·∫•y to√†n b·ªô buffer l√†m sequence
            raw_seq = np.array(list(self.realtime_buffer))
            
            # Scale d·ªØ li·ªáu (D√πng scaler ƒë√£ fit l√∫c train, ho·∫∑c partial_fit n·∫øu c·∫ßn)
            # ·ªû ƒë√¢y gi·∫£ ƒë·ªãnh scaler ƒë√£ ƒë∆∞·ª£c fit ho·∫∑c d√πng range m·∫∑c ƒë·ªãnh
            seq_scaled = self.scaler.transform(raw_seq)
            
            # Reshape (1, look_back, 5)
            input_reshaped = np.reshape(seq_scaled, (1, self.look_back, 5))
            
            # 3. D·ª± ƒëo√°n
            prediction = self.model.predict(input_reshaped, verbose=0)
            risk_score = float(prediction[0][0])
            
            return risk_score
            
        except Exception as e:
            print(f"LSTM Error: {e}")
            return 0.5

# Singleton
guardian_brain = DeepGuardian()