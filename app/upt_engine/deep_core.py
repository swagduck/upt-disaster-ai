import numpy as np
import tensorflow as tf
import os
from sklearn.preprocessing import MinMaxScaler

# Import k·∫øt n·ªëi Database
from app.core.database import Database

class DeepGuardian:
    def __init__(self):
        self.model_path = "app/upt_engine/guardian_lstm.keras"
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.look_back = 5 # AI s·∫Ω nh√¨n l·∫°i 5 s·ª± ki·ªán g·∫ßn nh·∫•t
        self.is_trained = False
        self.model = None
        
        # Ki·ªÉm tra GPU
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"üöÄ [DEEP CORE] NVIDIA GPU Detected: {len(gpus)} device(s).")
        else:
            print("‚ö†Ô∏è [DEEP CORE] GPU not found (or unsupported on native Windows). Running on CPU.")

        # Ki·∫øn tr√∫c m·∫°ng N∆°-ron
        self._build_brain()
        
        # Kh·ªüi ƒë·ªông: Train t·ª´ DB n·∫øu c√≥
        if Database.db is not None:
            print("üß† [DEEP CORE] Initializing Neural Network (LSTM)...")
            count = self.train_from_memory()
            if count == 0:
                print("‚ö†Ô∏è [DEEP CORE] Insufficient data history. Standing by for live data...")
        
    def _build_brain(self):
        """
        X√¢y d·ª±ng ki·∫øn tr√∫c m·∫°ng n∆°-ron LSTM.
        S·ª≠ d·ª•ng tf.keras tr·ª±c ti·∫øp ƒë·ªÉ tr√°nh l·ªói c·∫£nh b√°o (l·ªói v√†ng) trong IDE.
        """
        self.model = tf.keras.models.Sequential()
        
        # Input Layer (D√πng tf.keras.layers.Input)
        self.model.add(tf.keras.layers.Input(shape=(self.look_back, 5)))
        
        # Hidden Layers (D√πng tf.keras.layers.LSTM)
        self.model.add(tf.keras.layers.LSTM(units=50, return_sequences=True))
        self.model.add(tf.keras.layers.Dropout(0.2))
        
        self.model.add(tf.keras.layers.LSTM(units=50))
        self.model.add(tf.keras.layers.Dropout(0.2))
        
        # Output Layer
        self.model.add(tf.keras.layers.Dense(1))
        
        self.model.compile(optimizer='adam', loss='mean_squared_error')

    def train_from_memory(self):
        """H·ªçc t·ª´ l·ªãch s·ª≠ MongoDB (Sequence Learning)"""
        col = Database.get_collection("raw_logs")
        if col is None: return 0
        
        # L·∫•y 2000 b·∫£n ghi g·∫ßn nh·∫•t
        try:
            logs = list(col.find().sort("timestamp", 1).limit(2000))
        except: return 0
        
        # C·∫ßn √≠t nh·∫•t (look_back + v√†i m·∫´u) ƒë·ªÉ train
        if len(logs) < self.look_back + 5: return 0
        
        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu chu·ªói (Time Series)
        time_series_data = []
        
        for log in logs:
            sensors = log.get('sensors_data', [])
            if not sensors: continue
            
            # T√≠nh trung b√¨nh c√°c ch·ªâ s·ªë to√†n c·∫ßu t·∫°i th·ªùi ƒëi·ªÉm T
            avg_energy = np.mean([s.get('energy_level', 0) for s in sensors])
            avg_anomaly = np.mean([s.get('anomaly_score', 0) for s in sensors])
            max_mag = log.get('max_magnitude', 0)
            event_count = log.get('total_events', 0)
            
            # Vector 5 chi·ªÅu: [Energy, Anomaly, MaxMag, Count, Bias]
            time_series_data.append([avg_energy, avg_anomaly, max_mag/10.0, event_count/100.0, 0.5])

        if len(time_series_data) < self.look_back + 5: return 0
        
        dataset = np.array(time_series_data)
        dataset_scaled = self.scaler.fit_transform(dataset)
        
        # 2. T·∫°o c·ª≠a s·ªï tr∆∞·ª£t (Sliding Window)
        X, y = [], []
        for i in range(self.look_back, len(dataset_scaled)):
            X.append(dataset_scaled[i-self.look_back:i, :]) 
            y.append(dataset_scaled[i, 2]) # D·ª± ƒëo√°n MaxMag (index 2)
            
        X, y = np.array(X), np.array(y)
        
        # 3. Hu·∫•n luy·ªán
        self.model.fit(X, y, epochs=5, batch_size=1, verbose=0)
        self.is_trained = True
        
        return len(X)

    def learn(self, sensors_data):
        """H√†m Wrapper ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi API c≈©"""
        return self.train_from_memory()

    def predict_risk(self, lat, lon, energy, anomaly):
        if not self.is_trained:
            # Fallback (Ch·∫ø ƒë·ªô ch·ªù)
            return (energy * 0.6) + (anomaly * 0.4)
            
        try:
            # Gi·∫£ l·∫≠p input (V√¨ ch∆∞a c√≥ c∆° ch·∫ø query history realtime cho t·ª´ng request)
            current_features = [energy, anomaly, energy, 0.5, 0.5]
            
            # Nh√¢n b·∫£n input hi·ªán t·∫°i th√†nh chu·ªói (cho demo)
            input_seq = np.array([current_features] * self.look_back)
            input_seq = self.scaler.transform(input_seq)
            
            input_reshaped = np.reshape(input_seq, (1, self.look_back, 5))
            
            prediction = self.model.predict(input_reshaped, verbose=0)
            return float(prediction[0][0])
            
        except Exception as e:
            print(f"LSTM Error: {e}")
            return 0.5

# Singleton Instance
guardian_brain = DeepGuardian()